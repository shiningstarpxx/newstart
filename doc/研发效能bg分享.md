#### 研发效能BG分享

#### authur: Pei, Xingxin

#### email: xingxinpei@gmail.com



### 正文

接下来每一段代表了一页PPT内容

#### 1

hi，各位小伙伴好，我是来自腾讯医疗资讯与服务部的michael，主要负责腾讯云医的研发。这次主要来和大家分享，云医DevOps落地实践的回顾，以及这个过程中，我们获得来自质量部，云四部，运营部等部门的帮助

#### 2

本次分享的主题分为两个部分，第一个部分是整体的背景，包括了业务背景，以及我们研发侧为什么需要联合各个兄弟部门来一起做些有趣的事情。

第二个部分是，我们整体落地的过程，大致分为了1.0， 2.0， 3.0 三个part。

下面我们先进入第一个部分，背景

#### 3

腾讯云医是基于微信小程序打造一款，服务于医生和患者的云上医疗平台，具体形式如右图所示，我们涵盖了整个问诊的流程，也包括了处方，购买药物，以及基于医生，患者的关系链体系的相关功能

#### 4

腾讯云医的整体后台情况呢，如右图所示，我们实现了整体的云原生体系。整体上呢，我们采用的是微服务架构模式，所以在业务从0到1，从小到大的过程中，天然有一个问题，就是微服务的数量增长很快。而且这去年这个时候，新组建的团队，几乎没有研效的体系，也没有服务整体可观测体系，我们面临的两个核心问题是，开发效率比较低，通常两个星期才有一次版本发布，而且一旦线上有问题，我们排查很困难，基本上都是排查+碰运气的方式登陆一个个服务查看日志

#### 5

我们针对我们的研发现状深入分析后发现，我们主要是以下四个问题

1. 缺乏开发规划，开发模式不统一，有人有自测环境，有人喜欢在测试环境验证，有人是分支发布，有人是主干发布；服务框架上也是各有特色； 
2. 没有CI/CD，测试，发布完全靠手动，尴尬的时候，测试同学测试了一天才发现测试的版本不对
3. 代码也不规范，风格千奇百怪， 互相修改的时候，目录都要翻一遍才知道怎么下手；另外，也没有门禁测试，偶现的接口没有向前兼容，在业务迭代多次后，已经成固定规律出现
4. 测试环境只有一套，大家排队等测试；同时，测试也完全依赖手工

#### 6

线上定位问题慢，主要是下面三个原因 

#### 7

背景聊完后，我们下面分三个版本迭代，来分享我们是如何一步步提升整体研发效率和质量的

#### 8

这两年效能在业界越来越火，公司和BG也都投入了不少人力物力来实践；整体上看，研发迭代从之前的瀑布模式，逐步过渡到了现在的devops模式，一个核心点就是开发，测试，部署越来越一体化，而不是分割的三个阶段。

而整个devops的阶段细分后，分成了下面个五个域

#### 9

我们的解决问题指导思想，就是分治；先把问题按域话费，按域分别解决后，然后再整体串起来，就可以比较好的完成一次大的迭代。整个过程中，我们主要是和质量部做了深入的合作，彼此依靠来更好的服务业务

整个问题按域拆解后，我们的面临的问题具体来看是



针对这些问题，我们的解决死了是

#### 10

开发域我们解决问题的手段是统一开发模式

#### 11

构建/部署域我们解决问题的手段是构建流水线

#### 12

测试域效能提升的手段是自动化拨测

#### 13

测试域效能提升的另一个手段是，测试环境建设

#### 14

测试环境建设的方案如图，

效果是

#### 15

运营域的提升，我们主要是依赖监控的建设

#### 16

整个第一阶段，在几个域各自做了一轮优化后，主要是做了以下几方面的工作

我们解决了***

#### 17

第一个阶段，其实是投入少，见效快的；紧接着，我们面临的问题是越来越困难的， 具体来看，每个域



#### 18

于是我们启动了研效2.0的建设，整体的思路是下面三个方向

合作上，我们配合了BG技术委员会下设的研发效能组，跟质量部，四部，运营部等兄弟部门，我们还同公司其他BG的兄弟部门，通过开源协同的方式，一起提升了云医的研发效能和质量

#### 19

开发域我们当时做了又一次抽象，我们把开发过程抽象为两个重要部分，一个是服务开发，一个是协议开发，整体两个都面临了同样的两个问题

一个是标准化

一个是自动化

#### 20

整个解决方案思路就是XAC

换个角度想，我们也是把部署阶段的事情，提前到了开发阶段解决

#### 21

具体来看，我们是怎么做服务标准化设计的

#### 22

标准出来之后，怎么解放大家，不要让大家手动的按照标准一条条照着执行，那么答案也就很显然了，脚手架

#### 23

前面提到的，部署域的问题能不能提前到开发域来解决，甚至让开发不需要太关心部署域复杂的操作，我们这里采用了集成流水线的方式

#### 24

集成到流水线的效果如图所示

#### 25

protode 标准化也实在必行，有多个不同的编译依赖，长期看很难维护；解决方案，

#### 26

proto编译统一后，发布也是一个难题，我们当时面临的问题是

#### 27

整个过程能不能自动化，尽可能去掉人工环节呢？ 如果采用proto as code的思路，proto更新时，自动触发服务发布。。。

中间当然，一个核心点，要对proto做比较强的cr

#### 28

在测试域上，针对前面提到的全量环境复制问题，我们也做了一版优化

#### 29

增量方案中的一个难点就是路由问题

#### 30

我们实现增量方案后的多测试环境效果图

#### 31

测试域的另外两个问题是，

#### 32

工业界对缺陷引入和解决成本做了一些统计

#### 33

重构的作者和业界的一些其他的敏捷布道师认为 

#### 34

单元测试部门，我们主要是把单元测试做为了我们流水线的门禁

#### 35

可以看到我们的单元测试用例，代码覆盖度都在稳步提升

#### 36

另外从统计上看，单测在流水线中拦截了很多次，也是事实上保障了研发的质量

#### 37

测试左移下一个阶段可以做的事情是CR

这个阶段也是需要大量的时间，人力投入，才能形成一个比较好的工程文化

#### 38

测试左移还可以提升的一个点是，自动化回归测试，还记得我们前面是在流水线中增加了门禁-拨测用例来做了质量的提升，但这里很快遇到了一个核心瓶颈，人力不足，不能大量招测试开发的同学来写业务的拨测用例

#### 39 

自动化回归测试；流量回放的思路

#### 40

流量回放的效果 

#### 41

运营域，我们之前提到过我们在运营域上通过接入cms，有了一定的指标体系积累

而微服务的三大基石是

#### 42

具体来看metrics的建设，1.0版本以后面临的问题是

#### 43

那么我们能不能建设一个更好更全的指标体系，我们也做了抽象分类

#### 44

最终，我们在我们的微服务体系上，以及oceanus上分别实现了

#### 45

在我们具体落地的过程中，我们还是面临一些挑战，主要原因是，没有grpc的最佳实践

#### 46

接入云监控后的效果图

#### 47

线上的监控指标体系建立后，我们面临的一些链路追踪问题

#### 48

选用了云APM，

服务也做了全面改造

#### 49

构建了统一的grpc应用框架

#### 50

效果

#### 51

三大基石的最后一个部分，logging的建设

#### 52

logging的收益

#### 53

可观测的总结

#### 54

业务变复杂后，需要数据建设

#### 55

业界是怎么做的，也就是co

#### 56

云原生-数据湖等

#### 57

数据体系建设效果 

#### 58

云医研效2。0建设效果

#### 59

局部问题解决后，我们其实事实上变成了 

dq过程中，最后还是有个conquer的过程

#### 60

也就是分割的各自域，需要一个底座把他们串起来

#### 61

coding应用管理接入后的效果

#### 62

目前我们是按域划分，一个域认为是一个应用接入的

#### 63

整个过程中，我们也是互相支持，

#### 64 

应用管理效果

#### 65

总结

#### 66

展望